{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbf2f33-bc32-41ec-8055-8f7946049f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, when, expr, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# MLflow 실험 설정 - Databricks 경로 형식으로 수정\n",
    "mlflow.set_experiment(\"/Users/jazzyrain722@naver.com/Customer_Segmentation_Discount_Sensitivity\")\n",
    "\n",
    "# 데이터를 Delta 테이블로 저장한 경로\n",
    "delta_path = \"/delta/customer_rfm_features\"\n",
    "\n",
    "# 전처리된 RFM 데이터 로드\n",
    "data = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# 필요한 컬럼들에 대해 결측치 제거\n",
    "data_filtered = data.na.drop()\n",
    "\n",
    "# avg_purchase_amount 컬럼을 계산 (구매 횟수가 0이 아닌 경우에만 계산)\n",
    "data_with_avg = data_filtered.withColumn(\n",
    "    \"avg_purchase_amount\", \n",
    "    when(col(\"frequency\") > 0, col(\"monetary\") / col(\"frequency\")).otherwise(0)\n",
    ")\n",
    "\n",
    "# 데이터에 할인 관련 컬럼이 없는 경우 생성 (예시 데이터)\n",
    "if \"discount_used\" not in data_with_avg.columns:\n",
    "    # 가정: 평균 구매액이 높은 고객은 할인을 적게 사용했을 가능성\n",
    "    data_with_avg = data_with_avg.withColumn(\n",
    "        \"discount_used\",\n",
    "        when(col(\"avg_purchase_amount\") > 100, 0).otherwise(1)\n",
    "    )\n",
    "\n",
    "if \"discount_sensitive\" not in data_with_avg.columns:\n",
    "    # 가정: 빈번하게 구매하면서 할인을 사용한 고객은 할인에 민감\n",
    "    data_with_avg = data_with_avg.withColumn(\n",
    "        \"discount_sensitive\",\n",
    "        when((col(\"frequency\") > 3) & (col(\"discount_used\") == 1), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "# 모델에 필요한 피처 컬럼 선택 (RFM 및 평균구매액)\n",
    "features_df = data_with_avg.select(\n",
    "    \"customer_id\", \"recency\", \"frequency\", \"monetary\", \"avg_purchase_amount\",\n",
    "    \"discount_used\", \"discount_sensitive\"\n",
    ")\n",
    "\n",
    "# 피처 벡터화\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"recency\", \"frequency\", \"monetary\", \"avg_purchase_amount\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "assembled_data = assembler.transform(features_df)\n",
    "\n",
    "# 스케일링 추가 (K-means는 거리 기반이므로 스케일링이 중요)\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(assembled_data)\n",
    "scaled_data = scaler_model.transform(assembled_data)\n",
    "\n",
    "# 영어 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# K-means 최적 k 찾기 (Elbow Method)\n",
    "with mlflow.start_run(run_name=\"find_optimal_k\"):\n",
    "    cost = []\n",
    "    ks = list(range(2, 10))\n",
    "    \n",
    "    for k in ks:\n",
    "        kmeans = KMeans(featuresCol='scaled_features', k=k, seed=42)\n",
    "        model = kmeans.fit(scaled_data)\n",
    "        cost.append(model.summary.trainingCost)\n",
    "        \n",
    "        # 로깅\n",
    "        mlflow.log_metric(f\"WSSSE_k{k}\", model.summary.trainingCost)\n",
    "    \n",
    "    # 그래프 출력\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(ks, cost, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('WSSSE')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.savefig(\"elbow_method.png\")\n",
    "    mlflow.log_artifact(\"elbow_method.png\")\n",
    "    \n",
    "    # 최적 k 자동 계산 (급격한 감소가 완만해지는 지점)\n",
    "    diff = np.diff(cost)\n",
    "    diff2 = np.diff(diff)\n",
    "    optimal_k = ks[np.argmax(diff2) + 1]\n",
    "    \n",
    "    print(f\"Optimal k value is {optimal_k}\")\n",
    "    mlflow.log_param(\"optimal_k\", optimal_k)\n",
    "\n",
    "# 최적 k로 K-means 클러스터링 수행\n",
    "with mlflow.start_run(run_name=\"customer_segmentation\"):\n",
    "    kmeans = KMeans(featuresCol='scaled_features', k=optimal_k, seed=42)\n",
    "    model = kmeans.fit(scaled_data)\n",
    "    \n",
    "    # 클러스터 예측 결과 추가\n",
    "    clustered_data = model.transform(scaled_data)\n",
    "    \n",
    "    # 클러스터 중심점 확인\n",
    "    centers = model.clusterCenters()\n",
    "    center_df = pd.DataFrame(centers, columns=[\"recency\", \"frequency\", \"monetary\", \"avg_purchase_amount\"])\n",
    "    print(\"Cluster Centers:\")\n",
    "    print(center_df)\n",
    "    \n",
    "    # 클러스터 특성 분석 및 세그먼트 이름 부여\n",
    "    # 클러스터 특성 분석을 위한 집계\n",
    "    cluster_analysis = clustered_data.groupBy(\"prediction\").agg(\n",
    "        {\"recency\": \"avg\", \"frequency\": \"avg\", \"monetary\": \"avg\", \"avg_purchase_amount\": \"avg\"}\n",
    "    ).orderBy(\"prediction\")\n",
    "    \n",
    "    # Spark DataFrame을 Pandas로 변환하여 세그먼트 정의를 더 쉽게 함\n",
    "    cluster_pd = cluster_analysis.toPandas()\n",
    "    \n",
    "    # 세그먼트 이름을 정의하는 함수 (영어로 변경)\n",
    "    def define_segment(row):\n",
    "        # 이 로직은 데이터의 특성에 맞게 조정해야 함\n",
    "        recency = row[\"avg(recency)\"]\n",
    "        frequency = row[\"avg(frequency)\"]\n",
    "        monetary = row[\"avg(monetary)\"]\n",
    "        \n",
    "        if frequency > cluster_pd[\"avg(frequency)\"].mean() and monetary > cluster_pd[\"avg(monetary)\"].mean():\n",
    "            if recency < cluster_pd[\"avg(recency)\"].mean():\n",
    "                return \"Loyal Customer\"  # 충성 고객\n",
    "            else:\n",
    "                return \"Dormant Customer\"  # 휴면 고객\n",
    "        elif frequency <= 1:\n",
    "            return \"One-time Buyer\"  # 일회성 구매 고객\n",
    "        elif monetary > cluster_pd[\"avg(monetary)\"].mean() and frequency <= cluster_pd[\"avg(frequency)\"].mean():\n",
    "            return \"High-value Occasional\"  # 고가치 간헐적 고객\n",
    "        elif recency < cluster_pd[\"avg(recency)\"].mean() and frequency <= cluster_pd[\"avg(frequency)\"].mean():\n",
    "            return \"Potential Growth\"  # 잠재 성장 고객\n",
    "        else:\n",
    "            return \"Regular Customer\"  # 일반 고객\n",
    "    \n",
    "    # 세그먼트 이름 적용\n",
    "    cluster_pd[\"segment\"] = cluster_pd.apply(define_segment, axis=1)\n",
    "    print(\"Segment Definitions:\")\n",
    "    print(cluster_pd)\n",
    "    \n",
    "    # 클러스터 시각화 (2D로 축소하여 시각화)\n",
    "    from pyspark.ml.feature import PCA\n",
    "    \n",
    "    # PCA로 2차원 축소\n",
    "    pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "    pca_model = pca.fit(clustered_data)\n",
    "    pca_result = pca_model.transform(clustered_data)\n",
    "    \n",
    "    # PCA 결과를 시각화용 Pandas로 변환\n",
    "    pca_pdf = pca_result.select(\"prediction\", \"pca_features\").toPandas()\n",
    "    pca_pdf[\"pca1\"] = pca_pdf[\"pca_features\"].apply(lambda x: float(x[0]))\n",
    "    pca_pdf[\"pca2\"] = pca_pdf[\"pca_features\"].apply(lambda x: float(x[1]))\n",
    "    \n",
    "    # 클러스터별 색상 지정\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(optimal_k):\n",
    "        subset = pca_pdf[pca_pdf[\"prediction\"] == i]\n",
    "        segment_name = cluster_pd.loc[i, 'segment'] if i < len(cluster_pd) else f\"Cluster {i}\"\n",
    "        plt.scatter(subset[\"pca1\"], subset[\"pca2\"], label=f\"Cluster {i}: {segment_name}\")\n",
    "    \n",
    "    plt.title(\"Customer Segments PCA Visualization\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"cluster_visualization.png\")\n",
    "    mlflow.log_artifact(\"cluster_visualization.png\")\n",
    "\n",
    "    # 세그먼트 매핑 정보를 Spark에 적용\n",
    "    segment_mapping = {row.prediction: row.segment for i, row in cluster_pd.iterrows()}\n",
    "    \n",
    "    # UDF를 사용하여 세그먼트 이름 추가\n",
    "    def map_segment(cluster_id):\n",
    "        return segment_mapping.get(cluster_id, \"Unknown\")\n",
    "    \n",
    "    segment_udf = udf(map_segment, StringType())\n",
    "    segmented_data = clustered_data.withColumn(\"segment\", segment_udf(col(\"prediction\")))\n",
    "    \n",
    "    # 로깅\n",
    "    mlflow.spark.log_model(model, \"kmeans_model\")\n",
    "    \n",
    "    # 세그먼트별 통계 계산\n",
    "    segment_stats = segmented_data.groupBy(\"segment\").agg(\n",
    "        {\"recency\": \"avg\", \"frequency\": \"avg\", \"monetary\": \"avg\", \n",
    "         \"avg_purchase_amount\": \"avg\", \"discount_used\": \"avg\", \"discount_sensitive\": \"avg\"}\n",
    "    )\n",
    "    \n",
    "    print(\"Segment Statistics:\")\n",
    "    segment_stats.show()\n",
    "\n",
    "# 할인 민감도 예측 모델 구축 (오류 수정)\n",
    "with mlflow.start_run(run_name=\"discount_sensitivity_prediction\"):\n",
    "    # 훈련/테스트 데이터 분할\n",
    "    train_data, test_data = segmented_data.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # 라벨 값 확인 및 필요시 변환 - 디버깅용\n",
    "    print(\"라벨 분포 확인:\")\n",
    "    train_data.groupBy(\"discount_sensitive\").count().orderBy(\"discount_sensitive\").show()\n",
    "    \n",
    "    # LogisticRegression 사용 (이진 분류에 더 적합할 수 있음)\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    \n",
    "    lr = LogisticRegression(\n",
    "        labelCol=\"discount_sensitive\",\n",
    "        featuresCol=\"scaled_features\",\n",
    "        predictionCol=\"rf_prediction\",\n",
    "        probabilityCol=\"rf_probability\",\n",
    "        rawPredictionCol=\"rf_rawPrediction\",\n",
    "        maxIter=20,\n",
    "        regParam=0.05\n",
    "    )\n",
    "    \n",
    "    # 파라미터 그리드 설정 및 교차 검증\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.01, 0.05, 0.1]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    # 평가자 수정 - MulticlassClassificationEvaluator 사용\n",
    "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"discount_sensitive\", \n",
    "        predictionCol=\"rf_prediction\",\n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    cv = CrossValidator(\n",
    "        estimator=lr,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=3\n",
    "    )\n",
    "    \n",
    "    # 모델 훈련\n",
    "    cv_model = cv.fit(train_data)\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # 테스트 데이터로 평가\n",
    "    predictions = best_model.transform(test_data)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"Model Accuracy: {accuracy}\")\n",
    "    \n",
    "    # ROC AUC도 계산 (필요시)\n",
    "    binary_evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"discount_sensitive\",\n",
    "        rawPredictionCol=\"rf_probability\",  # probability 컬럼 사용\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    try:\n",
    "        auc = binary_evaluator.evaluate(predictions)\n",
    "        print(f\"Area under ROC: {auc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ROC AUC 계산 오류: {e}\")\n",
    "        auc = 0.0\n",
    "    \n",
    "    # 로깅\n",
    "    mlflow.spark.log_model(best_model, \"lr_model\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    if auc > 0:\n",
    "        mlflow.log_metric(\"auc\", auc)\n",
    "    \n",
    "    # 피처 중요도 확인 (로지스틱 회귀에서는 coefficients 사용)\n",
    "    coefficients = best_model.coefficients\n",
    "    feature_names = [\"recency\", \"frequency\", \"monetary\", \"avg_purchase_amount\"]\n",
    "    \n",
    "    # 계수의 절대값을 기준으로 중요도 계산\n",
    "    coef_abs = np.abs(coefficients.toArray())\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients.toArray(),\n",
    "        'Importance': coef_abs\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # 계수의 절대값 기준 중요도 시각화\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    plt.xlabel('Coefficient Magnitude (Absolute Value)')\n",
    "    plt.title('Feature Importance for Discount Sensitivity (Logistic Regression)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\")\n",
    "    mlflow.log_artifact(\"feature_importance.png\")\n",
    "    \n",
    "    # 계수 자체도 시각화 (부호 포함)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(importance_df['Feature'], importance_df['Coefficient'])\n",
    "    # 음수와 양수 계수에 다른 색상 적용\n",
    "    for i, bar in enumerate(bars):\n",
    "        if importance_df['Coefficient'].iloc[i] < 0:\n",
    "            bar.set_color('red')\n",
    "        else:\n",
    "            bar.set_color('green')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Feature Coefficients (Logistic Regression)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_coefficients.png\")\n",
    "    mlflow.log_artifact(\"feature_coefficients.png\")\n",
    "    \n",
    "    # 디버깅 - 확률 벡터의 형태 확인\n",
    "    predictions.select(\"rf_probability\").limit(5).show(truncate=False)\n",
    "    \n",
    "    # ROC 커브 시각화 - 수정된 접근 방식\n",
    "    # 확률 벡터의 구조를 확인하고 적절히 처리\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import DoubleType\n",
    "    \n",
    "    # 확률 벡터에서 클래스 1의 확률 추출하는 UDF\n",
    "    def get_class1_probability(prob_vector):\n",
    "        # 벡터 길이가 2인 경우 (일반적인 이진 분류)\n",
    "        if len(prob_vector) == 2:\n",
    "            return float(prob_vector[1])\n",
    "        # 벡터 길이가 1인 경우 (일부 모델에서 발생)\n",
    "        elif len(prob_vector) == 1:\n",
    "            return float(prob_vector[0])\n",
    "        else:\n",
    "            return 0.5  # 기본값\n",
    "    \n",
    "    get_prob_udf = udf(get_class1_probability, DoubleType())\n",
    "    \n",
    "    # 확률 추출하여 새 컬럼 생성\n",
    "    predictions_with_prob = predictions.withColumn(\"class1_probability\", get_prob_udf(\"rf_probability\"))\n",
    "    \n",
    "    # Pandas로 변환\n",
    "    pred_df = predictions_with_prob.select(\"class1_probability\", \"discount_sensitive\").toPandas()\n",
    "    y_true = pred_df[\"discount_sensitive\"].values\n",
    "    y_score = pred_df[\"class1_probability\"].values\n",
    "    \n",
    "    # ROC 커브 계산\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = sklearn_auc(fpr, tpr)\n",
    "    \n",
    "    # ROC 그래프 그리기\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(\"roc_curve.png\")\n",
    "    mlflow.log_artifact(\"roc_curve.png\")\n",
    "    \n",
    "    # 혼동 행렬 시각화 - 변환된 예측과 라벨 사용\n",
    "    pred_df_cm = predictions.select(\"rf_prediction\", \"discount_sensitive\").toPandas()\n",
    "    y_true_cm = pred_df_cm[\"discount_sensitive\"].values\n",
    "    y_pred_cm = pred_df_cm[\"rf_prediction\"].values\n",
    "    \n",
    "    # 혼동 행렬 계산\n",
    "    cm = confusion_matrix(y_true_cm, y_pred_cm)\n",
    "    \n",
    "    # 혼동 행렬 그래프 그리기\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Not Sensitive (0)', 'Sensitive (1)'])\n",
    "    plt.yticks(tick_marks, ['Not Sensitive (0)', 'Sensitive (1)'])\n",
    "    \n",
    "    # 값 표시\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "    \n",
    "    # 정확도, 정밀도, 재현율 계산\n",
    "    accuracy = accuracy_score(y_true_cm, y_pred_cm)\n",
    "    precision = precision_score(y_true_cm, y_pred_cm, zero_division=0)\n",
    "    recall = recall_score(y_true_cm, y_pred_cm, zero_division=0)\n",
    "    f1 = f1_score(y_true_cm, y_pred_cm, zero_division=0)\n",
    "    \n",
    "    # 메트릭 로깅\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1\", f1)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # 세그먼트별 할인 민감도 및 피쳐 영향 분석\n",
    "    # 앞서 생성한 class1_probability 컬럼 사용\n",
    "    segment_impact = predictions_with_prob.groupBy(\"segment\").agg(\n",
    "        {\"class1_probability\": \"avg\", \"rf_prediction\": \"avg\", \"discount_sensitive\": \"avg\"}\n",
    "    )\n",
    "    print(\"Segment Discount Sensitivity:\")\n",
    "    segment_impact.show()\n",
    "    \n",
    "    # 세그먼트별 예측 정확도 시각화\n",
    "    segment_accuracy = predictions_with_prob.withColumn(\n",
    "        \"correct_prediction\", \n",
    "        when(col(\"rf_prediction\") == col(\"discount_sensitive\"), 1).otherwise(0)\n",
    "    ).groupBy(\"segment\").agg(\n",
    "        {\"correct_prediction\": \"avg\", \"discount_sensitive\": \"avg\", \"prediction\": \"avg\"}\n",
    "    ).withColumnRenamed(\"avg(correct_prediction)\", \"accuracy\")\n",
    "    \n",
    "    # Pandas로 변환\n",
    "    segment_acc_pd = segment_accuracy.toPandas()\n",
    "    \n",
    "    # 세그먼트별 시각화 그래프\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # 세그먼트별 정확도\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(segment_acc_pd[\"segment\"], segment_acc_pd[\"accuracy\"], color='skyblue')\n",
    "    plt.xlabel('Customer Segment')\n",
    "    plt.ylabel('Prediction Accuracy')\n",
    "    plt.title('Discount Sensitivity Prediction Accuracy by Segment')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # 세그먼트별 할인 민감도 비율\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(segment_acc_pd[\"segment\"], segment_acc_pd[\"avg(discount_sensitive)\"], color='orange')\n",
    "    plt.xlabel('Customer Segment')\n",
    "    plt.ylabel('Discount Sensitivity Ratio')\n",
    "    plt.title('Actual Discount Sensitivity by Segment')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"segment_accuracy.png\")\n",
    "    mlflow.log_artifact(\"segment_accuracy.png\")\n",
    "    \n",
    "    # 모델 성능 종합 대시보드 생성\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # 1. 혼동 행렬\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Not Sensitive', 'Sensitive'])\n",
    "    plt.yticks(tick_marks, ['Not Sensitive', 'Sensitive'])\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    # 2. ROC 커브\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # 3. 피처 중요도\n",
    "    plt.subplot(2, 3, 3)\n",
    "    bars = plt.barh(importance_df['Feature'], importance_df['Coefficient'])\n",
    "    # 음수와 양수 계수에 다른 색상 적용\n",
    "    for i, bar in enumerate(bars):\n",
    "        if importance_df['Coefficient'].iloc[i] < 0:\n",
    "            bar.set_color('red')\n",
    "        else:\n",
    "            bar.set_color('green')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.title('Feature Coefficients')\n",
    "    \n",
    "    # 4. 세그먼트별 정확도\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.bar(segment_acc_pd[\"segment\"], segment_acc_pd[\"accuracy\"], color='skyblue')\n",
    "    plt.xlabel('Customer Segment')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Prediction Accuracy by Segment')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # 5. 세그먼트별 할인 민감도\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.bar(segment_acc_pd[\"segment\"], segment_acc_pd[\"avg(discount_sensitive)\"], color='orange')\n",
    "    plt.xlabel('Customer Segment')\n",
    "    plt.ylabel('Sensitivity Ratio')\n",
    "    plt.title('Discount Sensitivity by Segment')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # 6. 성능 메트릭\n",
    "    plt.subplot(2, 3, 6)\n",
    "    metrics = [accuracy, precision, recall, f1, roc_auc]\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "    plt.bar(metric_names, metrics, color='green')\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Metrics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_performance_dashboard.png\")\n",
    "    mlflow.log_artifact(\"model_performance_dashboard.png\")\n",
    "    print(\"Model performance visualization completed.\")\n",
    "\n",
    "# 최종 결과 저장 (영어로 수정)\n",
    "final_output = segmented_data.withColumn(\n",
    "    \"discount_recommendation\", \n",
    "    when(col(\"segment\") == \"Loyal Customer\", \"Low Discount (Retention Strategy)\")\n",
    "    .when(col(\"segment\") == \"Potential Growth\", \"Medium Discount (Growth Promotion)\")\n",
    "    .when(col(\"segment\") == \"One-time Buyer\", \"High Discount (Repurchase Incentive)\")\n",
    "    .when(col(\"segment\") == \"Dormant Customer\", \"High Discount + Personalization (Reactivation)\")\n",
    "    .otherwise(\"Medium Discount (General)\")\n",
    ")\n",
    "\n",
    "# Delta 테이블로 저장\n",
    "final_output.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/customer_segments_with_recommendations\")\n",
    "\n",
    "# 한국어로 모델 결과 요약 출력\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"고객 세그먼트 및 할인 민감도 분석 결과 요약\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 모델 성능 요약\n",
    "print(\"\\n[모델 성능 요약]\")\n",
    "print(f\"• 정확도(Accuracy): {accuracy:.4f} - 모델이 고객의 할인 민감도를 예측한 정확도입니다.\")\n",
    "print(f\"• 정밀도(Precision): {precision:.4f} - 모델이 '할인에 민감하다'고 예측한 고객 중 실제로 할인에 민감한 비율입니다.\")\n",
    "print(f\"• 재현율(Recall): {recall:.4f} - 실제 할인에 민감한 고객 중 모델이 정확히 식별한 비율입니다.\")\n",
    "print(f\"• F1 점수: {f1:.4f} - 정밀도와 재현율의 조화평균으로, 모델의 전반적인 성능을 나타냅니다.\")\n",
    "if auc > 0:\n",
    "    print(f\"• AUC: {auc:.4f} - 모델의 분류 성능을 나타내는 지표로, 1에 가까울수록 좋은 성능입니다.\")\n",
    "\n",
    "# 피처 중요도 요약\n",
    "print(\"\\n[피처 중요도 분석]\")\n",
    "for index, row in importance_df.head(4).iterrows():\n",
    "    feature = row['Feature']\n",
    "    coef = row['Coefficient']\n",
    "    importance = row['Importance']\n",
    "    effect = \"증가\" if coef > 0 else \"감소\"\n",
    "    \n",
    "    # 피처 이름 한글화\n",
    "    if feature == \"recency\":\n",
    "        feature_kr = \"최근성(최근 구매 일자)\"\n",
    "    elif feature == \"frequency\":\n",
    "        feature_kr = \"구매 빈도\"\n",
    "    elif feature == \"monetary\":\n",
    "        feature_kr = \"총 구매 금액\"\n",
    "    elif feature == \"avg_purchase_amount\":\n",
    "        feature_kr = \"평균 구매 금액\"\n",
    "    else:\n",
    "        feature_kr = feature\n",
    "        \n",
    "    print(f\"• {feature_kr}: 계수 = {coef:.4f}, 중요도 = {importance:.4f}\")\n",
    "    print(f\"  → 이 값이 증가하면 할인 민감도가 {effect}합니다.\")\n",
    "\n",
    "# 세그먼트별 할인 민감도 요약\n",
    "print(\"\\n[고객 세그먼트별 할인 민감도]\")\n",
    "segment_impact_pd = segment_impact.toPandas()\n",
    "for index, row in segment_impact_pd.iterrows():\n",
    "    segment = row['segment']\n",
    "    sensitivity = row['avg(discount_sensitive)']\n",
    "    prediction = row['avg(rf_prediction)']\n",
    "    \n",
    "    # 세그먼트 이름 한글화\n",
    "    if segment == \"Loyal Customer\":\n",
    "        segment_kr = \"충성 고객\"\n",
    "    elif segment == \"Potential Growth\":\n",
    "        segment_kr = \"잠재 성장 고객\"\n",
    "    elif segment == \"One-time Buyer\":\n",
    "        segment_kr = \"일회성 구매 고객\"\n",
    "    elif segment == \"Dormant Customer\":\n",
    "        segment_kr = \"휴면 고객\"\n",
    "    elif segment == \"High-value Occasional\":\n",
    "        segment_kr = \"고가치 간헐적 고객\"\n",
    "    elif segment == \"Regular Customer\":\n",
    "        segment_kr = \"일반 고객\"\n",
    "    else:\n",
    "        segment_kr = segment\n",
    "        \n",
    "    sensitivity_level = \"높음\" if sensitivity > 0.5 else \"낮음\"\n",
    "    print(f\"• {segment_kr}: 할인 민감도 = {sensitivity:.4f} ({sensitivity_level})\")\n",
    "    print(f\"  → 모델 예측 민감도 = {prediction:.4f}\")\n",
    "\n",
    "# 비즈니스 인사이트 및 결론\n",
    "print(\"\\n[비즈니스 인사이트 및 결론]\")\n",
    "print(\"• 이 분석은 고객 데이터를 RFM(Recency, Frequency, Monetary) 기준으로 세분화하고,\")\n",
    "print(\"  각 세그먼트별 할인 민감도를 예측하는 머신러닝 모델을 구축했습니다.\")\n",
    "print(\"• 이를 통해 마케팅팀은 고객 세그먼트별로 최적화된 할인 전략을 수립할 수 있습니다.\")\n",
    "print(\"• 예를 들어, 할인에 민감한 세그먼트에는 더 공격적인 할인 프로모션을,\")\n",
    "print(\"  할인에 덜 민감한 충성 고객에게는 할인보다 다른 혜택을 제공하는 전략이 효과적일 수 있습니다.\")\n",
    "print(\"• 이 모델은 데이터 기반의 개인화된 마케팅 전략 수립에 도움을 줄 수 있으며,\")\n",
    "print(\"  궁극적으로 고객 가치 극대화와 마케팅 비용 최적화에 기여할 수 있습니다.\")\n",
    "\n",
    "print(\"\\n[모델 한계 및 향후 개선 방향]\")\n",
    "print(\"• 현재 모델은 제한된 피처를 사용하고 있으며, 할인 민감도에 대한 실제 행동 데이터가 부족합니다.\")\n",
    "print(\"• 향후 A/B 테스트를 통한 실제 할인 반응 데이터를 수집하여 모델을 개선할 수 있습니다.\")\n",
    "print(\"• 또한 계절성, 제품 카테고리별 선호도, 인구통계학적 정보 등 추가 피처를 모델에 포함하면\")\n",
    "print(\"  예측 정확도를 더욱 향상시킬 수 있을 것입니다.\")\n",
    "\n",
    "print(\"\\n고객 세그먼트 및 할인 민감도 분석이 완료되었습니다.\")\n",
    "print(\"결과는 '/delta/customer_segments_with_recommendations'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b164e9-0934-4551-9dfe-1069e9c46416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 디버깅을 위한 임시 코드 - 실행 전에 segment_impact DataFrame 확인\n",
    "print(\"segment_impact 컬럼 확인:\")\n",
    "segment_impact.printSchema()\n",
    "segment_impact.show(5, truncate=False)\n",
    "\n",
    "# 디버깅용 파일로 저장\n",
    "try:\n",
    "    import sys\n",
    "    \n",
    "    # 파일로 결과 저장 (인코딩 지정)\n",
    "    with open(\"/dbfs/FileStore/analysis_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"고객 세그먼트 및 할인 민감도 분석 결과 요약\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # 모델 성능 요약\n",
    "        f.write(\"\\n[모델 성능 요약]\\n\")\n",
    "        f.write(f\"• 정확도(Accuracy): {accuracy:.4f} - 모델이 고객의 할인 민감도를 예측한 정확도입니다.\\n\")\n",
    "        f.write(f\"• 정밀도(Precision): {precision:.4f} - 모델이 '할인에 민감하다'고 예측한 고객 중 실제로 할인에 민감한 비율입니다.\\n\")\n",
    "        f.write(f\"• 재현율(Recall): {recall:.4f} - 실제 할인에 민감한 고객 중 모델이 정확히 식별한 비율입니다.\\n\")\n",
    "        f.write(f\"• F1 점수: {f1:.4f} - 정밀도와 재현율의 조화평균으로, 모델의 전반적인 성능을 나타냅니다.\\n\")\n",
    "        \n",
    "        # 세그먼트별 할인 민감도 요약 - 오류 방지용 try/except 구문 사용\n",
    "        f.write(\"\\n[고객 세그먼트별 할인 민감도]\\n\")\n",
    "        try:\n",
    "            # 안전하게 DataFrame 변환\n",
    "            segment_impact_pd = segment_impact.toPandas()\n",
    "            print(f\"segment_impact_pd 변환 성공: {len(segment_impact_pd)} 행\")\n",
    "            \n",
    "            # 컬럼명 확인 및 출력\n",
    "            print(f\"segment_impact_pd 컬럼: {segment_impact_pd.columns.tolist()}\")\n",
    "            \n",
    "            for index, row in segment_impact_pd.iterrows():\n",
    "                # 컬럼 이름이 다를 수 있으므로 안전하게 접근\n",
    "                segment = row.get('segment', 'Unknown')\n",
    "                \n",
    "                # 컬럼명 확인 및 예외 처리\n",
    "                sensitivity_col = 'avg(discount_sensitive)' if 'avg(discount_sensitive)' in row else 'avg(discount_sensitive)'\n",
    "                prediction_col = 'avg(rf_prediction)' if 'avg(rf_prediction)' in row else 'avg(rf_prediction)'\n",
    "                \n",
    "                sensitivity = row.get(sensitivity_col, 0.0)\n",
    "                prediction = row.get(prediction_col, 0.0)\n",
    "                \n",
    "                # 세그먼트 이름 한글화\n",
    "                segment_kr = segment\n",
    "                if segment == \"Loyal Customer\": segment_kr = \"충성 고객\"\n",
    "                elif segment == \"Potential Growth\": segment_kr = \"잠재 성장 고객\"\n",
    "                elif segment == \"One-time Buyer\": segment_kr = \"일회성 구매 고객\"\n",
    "                elif segment == \"Dormant Customer\": segment_kr = \"휴면 고객\"\n",
    "                elif segment == \"High-value Occasional\": segment_kr = \"고가치 간헐적 고객\"\n",
    "                elif segment == \"Regular Customer\": segment_kr = \"일반 고객\"\n",
    "                \n",
    "                sensitivity_level = \"높음\" if sensitivity > 0.5 else \"낮음\"\n",
    "                f.write(f\"• {segment_kr}: 할인 민감도 = {sensitivity:.4f} ({sensitivity_level})\\n\")\n",
    "                f.write(f\"  → 모델 예측 민감도 = {prediction:.4f}\\n\")\n",
    "        except Exception as e:\n",
    "            f.write(f\"세그먼트 데이터 처리 중 오류 발생: {str(e)}\\n\")\n",
    "        \n",
    "        # 비즈니스 인사이트 및 결론\n",
    "        f.write(\"\\n[비즈니스 인사이트 및 결론]\\n\")\n",
    "        f.write(\"• 이 분석은 고객 데이터를 RFM(Recency, Frequency, Monetary) 기준으로 세분화하고,\\n\")\n",
    "        f.write(\"  각 세그먼트별 할인 민감도를 예측하는 머신러닝 모델을 구축했습니다.\\n\")\n",
    "        f.write(\"• 이를 통해 마케팅팀은 고객 세그먼트별로 최적화된 할인 전략을 수립할 수 있습니다.\\n\")\n",
    "        f.write(\"• 예를 들어, 할인에 민감한 세그먼트에는 더 공격적인 할인 프로모션을,\\n\")\n",
    "        f.write(\"  할인에 덜 민감한 충성 고객에게는 할인보다 다른 혜택을 제공하는 전략이 효과적일 수 있습니다.\\n\")\n",
    "        \n",
    "    print(\"분석 결과가 '/dbfs/FileStore/analysis_results.txt'에 저장되었습니다.\")\n",
    "    \n",
    "    # Databricks 디스플레이 사용\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # 결과를 DataFrame으로 변환하여 표시\n",
    "    with open(\"/dbfs/FileStore/analysis_results.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        result_text = f.read()\n",
    "    \n",
    "    result_df = spark.createDataFrame([(result_text,)], [\"analysis_results\"])\n",
    "    display(result_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"결과 저장 중 오류 발생: {str(e)}\")\n",
    "\n",
    "# 출력 버퍼 강제 비우기\n",
    "import sys\n",
    "sys.stdout.flush()\n",
    "\n",
    "print(\"고객 세그먼트 및 할인 민감도 분석이 완료되었습니다.\")\n",
    "print(\"결과는 '/delta/customer_segments_with_recommendations'에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "머신러닝",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
